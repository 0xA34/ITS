# Baseline Evaluation Module

## M√¥-ƒëun ƒê√°nh gi√° C∆° s·ªü cho D·ª± √°n Kh·∫£o s√°t Vehicle Detection & Monitoring

Module n√†y cung c·∫•p c√°c c√¥ng c·ª• ƒë·ªÉ ƒë√°nh gi√° baseline cho h·ªá th·ªëng ph√°t hi·ªán v√† gi√°m s√°t ph∆∞∆°ng ti·ªán giao th√¥ng.

---

## üìÅ C·∫•u tr√∫c Th∆∞ m·ª•c

```
baseline/
‚îú‚îÄ‚îÄ __init__.py              # Module initialization
‚îú‚îÄ‚îÄ config.py                # Configuration settings
‚îú‚îÄ‚îÄ run_baseline.py          # Main evaluation runner
‚îú‚îÄ‚îÄ README.md                # This file
‚îÇ
‚îú‚îÄ‚îÄ datasets/                # Ground truth dataset loaders
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_dataset.py      # Base dataset class
‚îÇ   ‚îú‚îÄ‚îÄ coco_loader.py       # COCO format loader
‚îÇ   ‚îî‚îÄ‚îÄ custom_loader.py     # Custom dataset loader (COCO/YOLO/VOC)
‚îÇ
‚îú‚îÄ‚îÄ metrics/                 # Evaluation metrics
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ iou.py               # IoU calculation (IoU, GIoU, DIoU, CIoU)
‚îÇ   ‚îú‚îÄ‚îÄ precision_recall.py  # Precision, Recall, F1-Score
‚îÇ   ‚îú‚îÄ‚îÄ map_calculator.py    # mAP calculation (mAP@0.5, mAP@[.5:.95])
‚îÇ   ‚îî‚îÄ‚îÄ fps_benchmark.py     # FPS/Latency benchmarking
‚îÇ
‚îú‚îÄ‚îÄ comparison/              # Model comparison tools
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model_registry.py    # Model registry and management
‚îÇ   ‚îú‚îÄ‚îÄ reference_database.py # Reference methods database
‚îÇ   ‚îî‚îÄ‚îÄ comparison_report.py # Report generation (MD, HTML, JSON)
‚îÇ
‚îî‚îÄ‚îÄ results/                 # Output directory for evaluation results
```

---

## üöÄ H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

### 1. C√†i ƒë·∫∑t Dependencies

```bash
pip install ultralytics numpy opencv-python pydantic
```

### 2. Chu·∫©n b·ªã Ground Truth Dataset

#### T·∫£i COCO Dataset (Khuy·∫øn ngh·ªã)

```bash
# Download COCO val2017 (5000 images)
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
wget http://images.cocodataset.org/zips/val2017.zip

# Extract
unzip annotations_trainval2017.zip -d data/coco
unzip val2017.zip -d data/coco
```

#### Ho·∫∑c t·∫°o Custom Dataset

```python
from app.baseline.datasets.custom_loader import create_empty_dataset

# T·∫°o dataset tr·ªëng v·ªõi c·∫•u tr√∫c COCO
create_empty_dataset(
    output_path="data/custom",
    annotation_format="coco",
    class_names=["person", "bicycle", "car", "motorcycle", "bus", "truck"]
)
```

### 3. Ch·∫°y Baseline Evaluation

#### S·ª≠ d·ª•ng Command Line

```bash
# Ch·∫°y ƒë·∫ßy ƒë·ªß evaluation
python -m app.baseline.run_baseline --model yolov8n.pt --dataset data/coco --output results/

# Ch·ªâ ch·∫°y FPS benchmark
python -m app.baseline.run_baseline --model yolov8n.pt --benchmark-only

# Ch·∫°y v·ªõi gi·ªõi h·∫°n s·ªë ·∫£nh (ƒë·ªÉ test nhanh)
python -m app.baseline.run_baseline --model yolov8n.pt --dataset data/coco --max-images 100
```

#### S·ª≠ d·ª•ng Python API

```python
from app.baseline.run_baseline import BaselineEvaluator, EvaluationConfig

# C·∫•u h√¨nh
config = EvaluationConfig(
    model_path="yolov8n.pt",
    dataset_path="data/coco",
    dataset_format="coco",
    device="cuda",
    benchmark_iterations=100,
)

# Ch·∫°y evaluation
evaluator = BaselineEvaluator(config)
results = evaluator.run_full_evaluation()

# T·∫°o b√°o c√°o
evaluator.generate_report(results, output_path="results/baseline")
```

---

## üìä C√°c Metrics ƒë∆∞·ª£c H·ªó tr·ª£

### Accuracy Metrics

| Metric | M√¥ t·∫£ | C√¥ng th·ª©c |
|--------|-------|-----------|
| **Precision** | T·ª∑ l·ªá detection ƒë√∫ng | TP / (TP + FP) |
| **Recall** | T·ª∑ l·ªá ph√°t hi·ªán ƒë∆∞·ª£c | TP / (TP + FN) |
| **F1-Score** | Harmonic mean c·ªßa P v√† R | 2 √ó P √ó R / (P + R) |
| **mAP@0.5** | Mean AP t·∫°i IoU=0.5 | Average of AP@0.5 across classes |
| **mAP@0.75** | Mean AP t·∫°i IoU=0.75 | Average of AP@0.75 across classes |
| **mAP@[.5:.95]** | COCO-style mAP | Average of AP@[0.5:0.95:0.05] |

### Speed Metrics

| Metric | M√¥ t·∫£ | ƒê∆°n v·ªã |
|--------|-------|--------|
| **FPS** | Frames per second | frames/s |
| **Latency** | Th·ªùi gian x·ª≠ l√Ω 1 frame | ms |
| **Throughput** | S·ªë frame x·ª≠ l√Ω / t·ªïng th·ªùi gian | frames/s |

### IoU Variants

- **IoU** - Intersection over Union (c∆° b·∫£n)
- **GIoU** - Generalized IoU (x·ª≠ l√Ω non-overlapping boxes)
- **DIoU** - Distance IoU (x√©t kho·∫£ng c√°ch t√¢m)
- **CIoU** - Complete IoU (x√©t c·∫£ aspect ratio)

---

## üìà Reference Methods Database

Module bao g·ªìm database c√°c ph∆∞∆°ng ph√°p tham chi·∫øu ƒë·ªÉ so s√°nh:

### YOLO Family
- YOLOv8 (n, s, m, l, x)
- YOLOv7 (tiny, base, X)
- YOLOv5 (n, s, m, l)
- YOLO-NAS (S, M, L)

### R-CNN Family
- Faster R-CNN (ResNet-50/101 + FPN)
- Cascade R-CNN
- Mask R-CNN

### Other Methods
- SSD300/512
- DETR / Deformable DETR
- RT-DETR
- FCOS
- CenterNet
- EfficientDet
- NanoDet

```python
from app.baseline.comparison.reference_database import ReferenceDatabase

# Xem t·∫•t c·∫£ reference methods
ReferenceDatabase.print_summary()

# L·∫•y methods real-time (>30 FPS)
realtime_methods = ReferenceDatabase.get_realtime_methods(min_fps=30)

# L·∫•y methods c√≥ accuracy cao
accurate_methods = ReferenceDatabase.get_high_accuracy_methods(min_map=0.5)

# L·∫•y Pareto-optimal methods (best accuracy-speed trade-off)
pareto_methods = ReferenceDatabase.get_pareto_optimal()
```

---

## üìù Output Reports

### ƒê·ªãnh d·∫°ng Output

1. **JSON** - D·ªØ li·ªáu raw ƒë·ªÉ x·ª≠ l√Ω ti·∫øp
2. **Markdown** - B√°o c√°o c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c
3. **HTML** - B√°o c√°o v·ªõi charts v√† styling

### V√≠ d·ª• C·∫•u tr√∫c B√°o c√°o

```
results/baseline/
‚îú‚îÄ‚îÄ baseline_report_yolov8n.json      # Raw results
‚îú‚îÄ‚îÄ baseline_report_yolov8n.md        # Markdown report
‚îú‚îÄ‚îÄ baseline_report_yolov8n.html      # HTML report with charts
‚îî‚îÄ‚îÄ results_yolov8n.json              # Detailed evaluation results
```

---

## üéØ Vehicle Classes

C√°c class ph∆∞∆°ng ti·ªán ƒë∆∞·ª£c ƒë√°nh gi√° (COCO format):

| Class ID | Class Name | M√¥ t·∫£ |
|----------|------------|-------|
| 0 | person | Ng∆∞·ªùi ƒëi b·ªô |
| 1 | bicycle | Xe ƒë·∫°p |
| 2 | car | √î t√¥ |
| 3 | motorcycle | Xe m√°y |
| 5 | bus | Xe bu√Ωt |
| 7 | truck | Xe t·∫£i |

---

## üîß Configuration Options

```python
@dataclass
class EvaluationConfig:
    # Model settings
    model_path: str = "yolov8n.pt"
    confidence_threshold: float = 0.25
    iou_threshold: float = 0.45
    device: str = "auto"  # 'cuda', 'cpu', 'auto'
    
    # Dataset settings
    dataset_path: str = ""
    dataset_format: str = "coco"  # 'coco', 'yolo', 'voc'
    dataset_split: str = "val"
    max_images: int = None  # Limit for quick testing
    
    # Evaluation settings
    iou_thresholds: list = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
    evaluate_per_class: bool = True
    evaluate_per_size: bool = True
    
    # Benchmark settings
    benchmark_iterations: int = 100
    benchmark_warmup: int = 10
    image_size: tuple = (640, 640)
    
    # Output settings
    output_dir: str = "results/baseline"
    report_formats: list = ["json", "markdown", "html"]
```

---

## üìö Examples

### V√≠ d·ª• 1: Quick Benchmark

```python
from app.baseline.metrics.fps_benchmark import benchmark_model

result = benchmark_model(
    model_path="yolov8n.pt",
    num_iterations=100,
    image_size=(640, 640),
    device="cuda"
)
print(result)
```

### V√≠ d·ª• 2: Calculate mAP

```python
from app.baseline.metrics.map_calculator import MAPCalculator
from app.baseline.metrics.precision_recall import Detection, GroundTruth

calculator = MAPCalculator(
    class_names={0: "person", 2: "car", 3: "motorcycle"}
)

# Add predictions and ground truths
calculator.add_prediction(bbox=(100, 100, 200, 200), class_id=2, confidence=0.9, image_id="img1")
calculator.add_ground_truth(bbox=(105, 105, 195, 195), class_id=2, image_id="img1")

# Calculate mAP
result = calculator.calculate_coco_map()
print(f"mAP@0.5: {result.map_50:.4f}")
print(f"mAP@[.5:.95]: {result.map_50_95:.4f}")
```

### V√≠ d·ª• 3: Compare Models

```python
from app.baseline.comparison.comparison_report import ComparisonReport
from app.baseline.comparison.reference_database import ReferenceDatabase

report = ComparisonReport(title="Vehicle Detection Comparison")

# Add your evaluated model
report.add_model_results(
    model_name="Our YOLOv8n",
    precision=0.85,
    recall=0.78,
)
report.models["Our YOLOv8n"].map_50 = 0.72
report.models["Our YOLOv8n"].fps = 150

# Add reference methods
for ref in ReferenceDatabase.YOLO_METHODS[:5]:
    report.add_reference_method(
        model_name=ref.name,
        map_50=ref.map_50,
        fps=ref.fps,
        source=ref.source
    )

# Generate reports
report.generate_report("results/comparison")
```

---

## üîó Related Documentation

- [Ultralytics YOLOv8](https://docs.ultralytics.com/)
- [COCO Dataset](https://cocodataset.org/)
- [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)
- [Object Detection Metrics](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173)
